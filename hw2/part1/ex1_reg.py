########################################################################
# comp 540                                                             #
# Statistical Machine Learning                                         #
# Devika Subramanian, Rice University                                  #
#                                                                      #
# Homework 2: Logistic regression                                      #
########################################################################

########################################################################
#  Instructions
#  ------------
# 
#  This file contains code that helps you get started on 
#  logistic regression. You will need to complete the functions 
#  in logistic_regressor.py and utils.py in the places indicated.
#
#  Regularized logistic regression
########################################################################
##================ Part 0: Reading data and plotting ==================#
########################################################################

import pandas as pd
import numpy as np

data = pd.read_csv('ex2data2.txt')
X = np.vstack([data.x1,data.x2]).T
y = data.y

import matplotlib.pyplot as plt
import plot_utils

print 'Plotting data with green circle indicating (y=1) examples and red circle indicating (y=0) examples ...'
plot_utils.plot_twoclass_data(X,y,'Chip Test 1', 'Chip Test 2',['y=0','y=1'])
plt.savefig('fig3.pdf')


########################################################################
##================ Part 1: Compute cost and gradient ==================#
########################################################################

# map the features in ex2data2.txt into a pth order polynomial

import sklearn
from sklearn.preprocessing import PolynomialFeatures

# Map X onto polynomial features and normalize

p = 6
poly = sklearn.preprocessing.PolynomialFeatures(degree=p,include_bias=False)
X_poly = poly.fit_transform(X)

# set up the data matrix (expanded basis functions) with the column of ones as intercept

XX = np.vstack([np.ones((X_poly.shape[0],)),X_poly.T]).T

# set up a regularized logistic regression model

from logistic_regressor import RegLogisticRegressor

reg_lr1 = RegLogisticRegressor()

# run fmin on the loss function and gradient implemented in logistic_regressor.py

reg = 1.0
theta_opt = reg_lr1.train(XX,y,reg=reg,num_iters=1000,norm=False)

# print the theta found and the final loss

print 'Theta found by fmin_bfgs: ',theta_opt
print "Final loss = ", reg_lr1.loss(theta_opt,XX,y,0.0)

# plot the decision boundary

plot_utils.plot_decision_boundary_poly(X,y,theta_opt,reg,p,'Chip Test 1', 'Chip Test 2',['y = 0','y = 1'])
plt.savefig('fig4.pdf')

# compute accuracy on training set

reg_lr1.theta = theta_opt
predy = reg_lr1.predict(XX)

# TODO: fill in the expression for accuracy of prediction
accuracy = (predy == y).sum() / float(y.size)
print "Accuracy on the training set = ", accuracy

##################################################################
## Underfit and overfit models                                  ##
##################################################################
reg_under = 10
reg_lr_under = RegLogisticRegressor()
theta_under = reg_lr_under.train(XX,y,reg=reg_under,num_iters=1000,norm=False)
plot_utils.plot_decision_boundary_poly(X,y,theta_under,reg_under,p,'Chip Test 1','Chip Test 2',['y = 0','y = 1'])
plt.savefig('fig4_underfit.pdf')

reg_over = 0
reg_lr_over = RegLogisticRegressor()
theta_over = reg_lr_under.train(XX,y,reg=reg_over,num_iters=1000,norm=False)
plot_utils.plot_decision_boundary_poly(X,y,theta_over,reg_over,p,'Chip Test 1','Chip Test 2',['y = 0','y = 1'])
plt.savefig('fig4_overfit.pdf')
##################################################################
##################################################################

# Compare with model learned by sklearn's logistic regression with reg = 1/C
# the regularization parameter set below can be varied (on a logarithmic scale)

reg = 1.0

# L2 regularization with sklearn LogisticRegression

from sklearn import linear_model
sk_logreg_l2 = linear_model.LogisticRegression(C=1.0/reg,solver='lbfgs',fit_intercept=False)
sk_logreg_l2.fit(XX,y)
print "Theta found by sklearn with L2 reg: ", sk_logreg_l2.coef_[0]
print "Loss with sklearn theta: ", reg_lr1.loss(sk_logreg_l2.coef_[0],XX,y,0.0)

plot_utils.plot_decision_boundary_sklearn_poly(X,y,sk_logreg_l2,reg,p,'Exam 1 score', 'Exam 2 score',['Not Admitted','Admitted'])
plt.savefig('fig4_sk.pdf')


# L1 regularization witk sklearn LogisticRegression

sk_logreg_l1 = linear_model.LogisticRegression(C=1.0/reg,solver='liblinear',fit_intercept=False,penalty='l1')
sk_logreg_l1.fit(XX,y)
print "Theta found by sklearn with L1 reg: ", sk_logreg_l1.coef_[0]
print "Loss with sklearn theta: ", reg_lr1.loss(sk_logreg_l1.coef_[0],XX,y,0.0)

# plot regularization paths for L1 regression
# Exploration of L1 regularization 
# 
plot_utils.plot_regularization_path(XX,y)
plt.savefig('fig5.pdf')
